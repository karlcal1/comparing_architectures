{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d622e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "heres what the dataframe looks like btw:\n",
      "   Time (ms)  Centroid_X  Centroid_Y  Feature\n",
      "0          0  -18.112448   -7.618048       39\n",
      "1          1  -13.046840   -9.590575       39\n",
      "2          2   -8.764083  -12.284308       39\n",
      "3          3   -2.110503  -12.777150       39\n",
      "4          4    1.029483  -13.182605       39\n",
      "5          5    5.663209  -10.902855       39\n",
      "6          6    6.456484   -9.254714       39\n",
      "7          7    2.735900   -3.856508       39\n",
      "8          8   -0.484103   -0.866501       39\n",
      "9          9   -5.195110    3.019698       39\n",
      "[-18.112448 -13.04684   -8.764083 ... -21.287073 -22.94848  -22.354143]\n",
      "[ -7.618048   -9.590575  -12.284308  ...  -1.4127754  -2.2394783\n",
      "  -5.196636 ]\n",
      "[[-18.112448   -7.618048 ]\n",
      " [-13.04684    -9.590575 ]\n",
      " [ -8.764083  -12.284308 ]\n",
      " ...\n",
      " [-14.897392   -6.7442827]\n",
      " [-12.214083   -5.9264235]\n",
      " [ -7.3841033  -5.4664307]]\n",
      "data length:  20000\n",
      "model class OurLSTM with gap = 5, window = 150, num_layers = 1 started Fri Dec 12 00:31:50 2025\n",
      "time = 1.6 s, epoch number 1, test rmse_x = 5.639 μm, test rmse_y = 8.381 μm\n",
      "time = 13.9 s, epoch number 11, test rmse_x = 4.212 μm, test rmse_y = 4.321 μm\n",
      "time = 26.4 s, epoch number 21, test rmse_x = 3.277 μm, test rmse_y = 3.639 μm\n",
      "time = 38.5 s, epoch number 31, test rmse_x = 2.931 μm, test rmse_y = 3.756 μm\n",
      "time = 51.4 s, epoch number 41, test rmse_x = 2.634 μm, test rmse_y = 3.047 μm\n",
      "time = 64.3 s, epoch number 51, test rmse_x = 2.498 μm, test rmse_y = 2.931 μm\n",
      "time = 77.2 s, epoch number 61, test rmse_x = 2.630 μm, test rmse_y = 2.847 μm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 131\u001b[39m\n\u001b[32m    129\u001b[39m our_model.train()\n\u001b[32m    130\u001b[39m torch.cuda.synchronize() \u001b[38;5;66;03m# for good measure\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m=\u001b[49m\u001b[43mour_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(\"pred.device\", pred.device)\u001b[39;49;00m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(\"target.device\", target.device)\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mOurJitterDataset.__getitem__\u001b[39m\u001b[34m(self, i)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28minput\u001b[39m= torch.tensor(\u001b[38;5;28mself\u001b[39m.data[i:i+\u001b[38;5;28mself\u001b[39m.window], dtype=torch.float32) \u001b[38;5;66;03m#thisll b 600,2\u001b[39;00m\n\u001b[32m     45\u001b[39m output=torch.tensor( \u001b[38;5;28mself\u001b[39m.data[i+\u001b[38;5;28mself\u001b[39m.window+\u001b[38;5;28mself\u001b[39m.gap], dtype=torch.float32) \u001b[38;5;66;03m#thisll b 2,. the output prediction\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# if GPU is available, move to GPU\u001b[39;00m\n\u001b[32m     47\u001b[39m output = output.to(device)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m,output\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#gpu acceleration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "#data loading\n",
    "df= dataframe = pd.read_feather(\"combined_centroid_data.feather\")\n",
    "x=df[\"Centroid_X\"].values #.values is a pd attribute #gta take .values attribute to remove all the indexing, just returns array\n",
    "y=df[\"Centroid_Y\"].values\n",
    "our_data= np.stack([x,y], axis=1) #stack invents a new axis, we tryna couple the [x_i y_i]\n",
    "our_data = reduced_size= our_data[:20000] #the paper didnt use all the gazilliopn timesteps, theu used 10**5 i think. but it'll take way to long to run for our purposes, like several days.\n",
    "print(\"heres what the dataframe looks like btw:\")\n",
    "print(df[:10])\n",
    "print(x)\n",
    "print(y)\n",
    "print(our_data)\n",
    "print(\"data length: \",len(our_data))\n",
    "\n",
    "#hyperparams to keep fixed\n",
    "epochs= 100 #if it doesnt converge, i gotta adjust\n",
    "batch_siz= 128 #just depending on speed mostly, big minib. faster\n",
    "lr=.002\n",
    "hidden=64 #REMEMBER THIS GOTTA B DIVIDISIBLE B nhead WHICH I PUT AS 4 FOR THRANDFORMER\n",
    "#lets create a class to return input and output data \n",
    "\n",
    "class OurJitterDataset():\n",
    "    def __init__(self,data,window,gap):\n",
    "        self.data=data\n",
    "        self.window=window\n",
    "        self.gap=gap\n",
    "\n",
    "    def __len__(self): #i use this s.t. i can run .random_split down below\n",
    "        return len(self.data)-self.window-self.gap #also this makes it so we dont go outside the data!\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        input= torch.tensor(self.data[i:i+self.window], dtype=torch.float32) #thisll b 600,2\n",
    "        output=torch.tensor( self.data[i+self.window+self.gap], dtype=torch.float32) #thisll b 2,. the output prediction\n",
    "        input = input.to(device) # if GPU is available, move to GPU\n",
    "        output = output.to(device)\n",
    "        return input,output\n",
    "    \n",
    "# MODELS ########################\n",
    "#### LSTM\n",
    "#lets roll out the model\n",
    "class OurLSTM(nn.Module):\n",
    "    def __init__(self, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm=nn.LSTM(2,hidden_size=hidden, num_layers=num_layers,batch_first=True)\n",
    "        self.out_proj=nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output_of_the_lstm,(final_hidden_state,more_irrelevant_stuff)= self.lstm(x)  #\n",
    "        return self.out_proj(final_hidden_state[-1]) #last layer hidden state, equiv to [0] for us cuz we just have 1 layer\n",
    "#### LSTM+CNN\n",
    "class OurCNNLSTM(nn.Module):\n",
    "    def __init__(self, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.conv= nn.Conv1d(2,32,kernel_size=5,padding=2)  \n",
    "        self.lstm= nn.LSTM(32,hidden_size=hidden,num_layers=num_layers,batch_first=True)\n",
    "        self.out_proj= nn.Linear(hidden, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x= x.permute(0,2,1)        \n",
    "        x= torch.relu(self.conv(x))  \n",
    "        x= x.permute(0,2,1)      \n",
    "        output_of_the_lstm,(final_hidden_state,more_irrelevant_stuff)= self.lstm(x)\n",
    "        return self.out_proj(final_hidden_state[-1])\n",
    "#### TRANSF\n",
    "class OurTransformer(nn.Module):\n",
    "    def __init__(self, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_proj=nn.Linear(2,hidden)\n",
    "        encoder_layer=nn.TransformerEncoderLayer(d_model=hidden,nhead=4,batch_first=True)\n",
    "        self.transformer= nn.TransformerEncoder(encoder_layer,num_layers=num_layers)\n",
    "        self.out_proj= nn.Linear(hidden,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x= self.input_proj(x)         \n",
    "        x= self.transformer(x)         \n",
    "        return self.out_proj(x[:, -1])   #last timestep\n",
    "                    \n",
    "\n",
    "\n",
    "#the 3 hyperparams to vary\n",
    "for gap in [5,10,20]:\n",
    "    for window in [150,300,600]:\n",
    "\n",
    "        #load up the data. we can put it here bc it only depends on gap and window, not num layers in the model\n",
    "        our_dataset = OurJitterDataset(our_data,window,gap)\n",
    "        train_size=int(.8*len(our_dataset))\n",
    "            \n",
    "        train_data,test_data= torch.utils.data.random_split(our_dataset,[train_size,len(our_dataset)-train_size])\n",
    "        train_loader= DataLoader(train_data,batch_size=batch_siz,shuffle=True)\n",
    "        test_loader= DataLoader(test_data,batch_size=batch_siz,shuffle=True)\n",
    "\n",
    "        for num_layers in [1,2,3]:\n",
    "        # for num_layers in [3,2,1]:\n",
    "            for model_class in [OurLSTM, OurCNNLSTM , OurTransformer]:\n",
    "            # for model_class in [OurTransformer, OurLSTM, OurCNNLSTM]:\n",
    "\n",
    "                our_model = model_class(hidden,num_layers) #instantiating and initializing the model based on the model class which we loop over\n",
    "                our_model = our_model.to(device) # passing model to GPU if available; else, CPU\n",
    "                torch.cuda.synchronize() # for good measure\n",
    "                # -------------------------------------------\n",
    "                ###########################################here!!!!!!!!!!\n",
    "                optimizer=torch.optim.Adam(our_model.parameters(),lr=lr)\n",
    "                loss_fn = nn.MSELoss()\n",
    "\n",
    "                train_rmse_list_x=[]\n",
    "                train_rmse_list_y=[]\n",
    "                test_rmse_list_x=[]\n",
    "                test_rmse_list_y=[]\n",
    "                train_loss_list=[]\n",
    "                test_loss_list=[]\n",
    "\n",
    "                #NOTICE! a thing to be mindful of here is that the model mse is taken per batch whereas rmse x list is taken per epoch... i will keep it like this because we dont need the model mse anyway, its just extra information, and rmse is more interpretable for our task anyway!\n",
    "\n",
    "                start = time.time()\n",
    "                print(f\"model class {model_class.__name__} with gap = {gap}, window = {window}, num_layers = {num_layers} started {time.asctime(time.localtime())}\")\n",
    "                for epoch in range(epochs):\n",
    "                    our_model.train()\n",
    "                    torch.cuda.synchronize() # for good measure\n",
    "                    for inp,target in train_loader:\n",
    "                        pred=our_model(inp)\n",
    "                        # print(\"pred.device\", pred.device)\n",
    "                        # print(\"target.device\", target.device)\n",
    "                        l=loss_fn(pred, target)\n",
    "                        train_loss_list.append(l.item()) #.item() gets the value... bc remember, l will have gradient attched to it\n",
    "                        optimizer.zero_grad()\n",
    "                        l.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "\n",
    "                    our_model.eval() #so we disable dropout\n",
    "                    \n",
    "                    train_errors=[] #this is rmse\n",
    "                    test_errors=[] #this is rmse\n",
    "\n",
    "                    #lets look at the rmse\n",
    "                    with torch.no_grad(): #just makes the runs a little faster by disabling the whole computational graph stuff. it matters alot actually when u run big models.\n",
    "                        for inp,target in train_loader:\n",
    "                            pred= our_model(inp)\n",
    "                            train_errors.append((pred-target).detach().cpu().numpy())\n",
    "                    train_errors=np.concatenate(train_errors)\n",
    "                    rmse_x= np.sqrt((train_errors[:,0]**2).mean())\n",
    "                    rmse_y= np.sqrt((train_errors[:,1]**2).mean())\n",
    "                    train_rmse_list_x.append(rmse_x)\n",
    "                    train_rmse_list_y.append(rmse_y)\n",
    "\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for inp,target in test_loader:\n",
    "                            pred= our_model(inp)\n",
    "                            test_errors.append((pred-target).detach().cpu().numpy())\n",
    "                            test_loss_list.append(loss_fn(pred,target).item())\n",
    "                    test_errors=np.concatenate(test_errors)\n",
    "                    rmse_x= np.sqrt((test_errors[:,0]**2).mean())\n",
    "                    rmse_y= np.sqrt((test_errors[:,1]**2).mean())\n",
    "                    test_rmse_list_x.append(rmse_x)\n",
    "                    test_rmse_list_y.append(rmse_y)\n",
    "                    if epoch%10==0:\n",
    "                        epoch_time = time.time() - start\n",
    "                        print(f\"time = {epoch_time:.1f} s, epoch number {epoch+1}, test rmse_x = {rmse_x:.3f} μm, test rmse_y = {rmse_y:.3f} μm\")\n",
    "\n",
    "                end = time.time()\n",
    "                print(f\"done {time.asctime(time.localtime())}\")\n",
    "                print(f\"runtime: {(end-start) // 60:.0f}:{(end-start) % 60:.0f}\") \n",
    "                fig=plt.figure()\n",
    "                ax=fig.add_subplot()\n",
    "                ax.plot(train_rmse_list_x,label=\"train x\")\n",
    "                ax.plot(train_rmse_list_y,label=\"train y\")\n",
    "                ax.plot(test_rmse_list_x,label=\"test x\")\n",
    "                ax.plot(test_rmse_list_y,label=\"test y\")\n",
    "                ax.legend()\n",
    "                ax.set_xlabel(\"training epoch\")\n",
    "                ax.set_ylabel(\"test loss rmse\")\n",
    "                ax.set_title(f\"rmse of testing data for x and y \\n {model_class.__name__}, gap = {gap}, window = {window}, num_layers= {num_layers}\")\n",
    "                \n",
    "                #all the plots!!!! all plots are gonna get saved into the directory, so the folder we store this file in \n",
    "                plt.savefig(f\"outputs/plot_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\")                        #turns out u gotta save before showing lest u get an empty plot. also use the name dunders to avoid those classic ugly brackets u get othewise\n",
    "                #lets save the trained model. lemme just give it unmistakeable names:\n",
    "                torch.save(our_model.state_dict(), f\"outputs/params_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\")\n",
    "\n",
    "\n",
    "                np.save(f\"outputs/train_rmse_x_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\", train_rmse_list_x)\n",
    "                np.save(f\"outputs/train_rmse_y_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\", train_rmse_list_y)\n",
    "                np.save(f\"outputs/test_rmse_x_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\", test_rmse_list_x)\n",
    "                np.save(f\"outputs/test_rmse_y_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\", test_rmse_list_y)\n",
    "                np.save(f\"outputs/train_loss_list_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\", train_loss_list) #we can also plot this later shud we find it interesting. but rn lets not overpower ourselves, rmse is what ewe care about anyway. also these guys are gonna be per batchw whereas the rmse guys are per epoch\n",
    "                np.save(f\"outputs/test_loss_list_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\", test_loss_list)\n",
    "\n",
    "                # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f06911",
   "metadata": {},
   "source": [
    "# below is the same code, except in the way we originally wrote it, which is ipynb format. code above however runs grid search loop, hence all in one cell (the in loop above also we have added a few things things like torch.save(), plt.save() etc.) also just load w np.load etc the saved stuff from above so we can put it in our paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d8d52",
   "metadata": {},
   "source": [
    "### experimenting with various architectures beyond MLP :-)\n",
    "##############################################################\n",
    "#### 1. LSTM\n",
    "#### 2. CNN + LSTM\n",
    "#### 3. Transformer\n",
    "##############################################################\n",
    "### for each of these, i'll vary: \n",
    "#### 1. num_layers\n",
    "#### 2. hidden size (aka dimensionality)\n",
    "#### 3. Input window length (150,300,600)... it could be that too many time-samples cause overfitting\n",
    "##############################################################\n",
    "#####   technically, we could do a grid search and try like 3 combos of each hyperparam, i.e. 3x3x3=27 runs for each architecture type\n",
    "#####   however, i'll just run \"fix and vary\". it's unlikely that there are interactions here between the hyperparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a04de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#data loading\n",
    "df= dataframe = pd.read_feather(\"combined_centroid_data.feather\")\n",
    "x=df[\"Centroid_X\"].values #.values is a pd attribute #gta take .values attribute to remove all the indexing, just returns array\n",
    "y=df[\"Centroid_Y\"].values\n",
    "our_data= np.stack([x,y], axis=1) #stack invents a new axis, we tryna couple the [x_i y_i]\n",
    "our_data = reduced_size= our_data[:10000] #the paper didnt use all the gazilliopn timesteps, theu used 10**5 i think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaeedb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heres what the dataframe looks like btw:\n",
      "   Time (ms)  Centroid_X  Centroid_Y  Feature\n",
      "0          0  -18.112448   -7.618048       39\n",
      "1          1  -13.046840   -9.590575       39\n",
      "2          2   -8.764083  -12.284308       39\n",
      "3          3   -2.110503  -12.777150       39\n",
      "4          4    1.029483  -13.182605       39\n",
      "5          5    5.663209  -10.902855       39\n",
      "6          6    6.456484   -9.254714       39\n",
      "7          7    2.735900   -3.856508       39\n",
      "8          8   -0.484103   -0.866501       39\n",
      "9          9   -5.195110    3.019698       39\n",
      "[-18.112448 -13.04684   -8.764083 ... -21.287073 -22.94848  -22.354143]\n",
      "[ -7.618048   -9.590575  -12.284308  ...  -1.4127754  -2.2394783\n",
      "  -5.196636 ]\n",
      "[[-18.112448    -7.618048  ]\n",
      " [-13.04684     -9.590575  ]\n",
      " [ -8.764083   -12.284308  ]\n",
      " ...\n",
      " [ -0.63192445   1.02285   ]\n",
      " [  3.8037598    1.02285   ]\n",
      " [  8.485918     0.28353426]]\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(\"heres what the dataframe looks like btw:\")\n",
    "print(df[:10])\n",
    "print(x)\n",
    "print(y)\n",
    "print(our_data)\n",
    "print(len(our_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams to keep fixed\n",
    "epochs= 100 #if it doesnt converge, i gotta adjust\n",
    "batch_siz= 128 #just depending on speed mostly, big minib. faster\n",
    "gap=20\n",
    "lr=.002\n",
    "\n",
    "#the 3 hyperparams to vary\n",
    "num_layers=3\n",
    "hidden=64 #REMEMBER THIS GOTTA B DIVIDISIBLE B nhead WHICH I PUT AS 4 FOR THRANDFORMER\n",
    "window=600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15289c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create a class to return input and output data \n",
    "\n",
    "class OurJitterDataset():\n",
    "    def __init__(self,data,window,gap):\n",
    "        self.data=data\n",
    "        self.window=window\n",
    "        self.gap=gap\n",
    "\n",
    "    def __len__(self): #i use this s.t. i can run .random_split down below\n",
    "        return len(self.data)-self.window-self.gap #also this makes it so we dont go outside the data!\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        input= torch.tensor(self.data[i:i+self.window], dtype=torch.float32) #thisll b 600,2\n",
    "        output=torch.tensor( self.data[i+self.window+self.gap], dtype=torch.float32) #thisll b 2,. the output prediction\n",
    "        return input,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33816122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load up the data\n",
    "our_dataset = OurJitterDataset(our_data,window,gap)\n",
    "train_size=int(.8*len(our_dataset))\n",
    "    \n",
    "train_data,test_data= torch.utils.data.random_split(our_dataset,[train_size,len(our_dataset)-train_size])\n",
    "train_loader= DataLoader(train_data,batch_size=batch_siz,shuffle=True)\n",
    "test_loader= DataLoader(test_data,batch_size=batch_siz,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc3b2f8",
   "metadata": {},
   "source": [
    "# MODELS ########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e44b7",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa6ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets roll out the model\n",
    "class OurLSTM(nn.Module):\n",
    "    def __init__(self, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm=nn.LSTM(2,hidden_size=hidden, num_layers=num_layers,batch_first=True)\n",
    "        self.out_proj=nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output_of_the_lstm,(final_hidden_state,more_irrelevant_stuff)= self.lstm(x)  #\n",
    "        return self.out_proj(final_hidden_state[-1]) #last layer hidden state, equiv to [0] for us cuz we just have 1 layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35bd576",
   "metadata": {},
   "source": [
    "#### LSTM+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe13b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurCNNLSTM(nn.Module):\n",
    "    def __init__(self, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.conv= nn.Conv1d(2,32,kernel_size=5,padding=2)  \n",
    "        self.lstm= nn.LSTM(32,hidden_size=hidden,num_layers=num_layers,batch_first=True)\n",
    "        self.out_proj= nn.Linear(hidden, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x= x.permute(0,2,1)        \n",
    "        x= torch.relu(self.conv(x))  \n",
    "        x= x.permute(0,2,1)      \n",
    "        output_of_the_lstm,(final_hidden_state,more_irrelevant_stuff)= self.lstm(x)\n",
    "        return self.out_proj(final_hidden_state[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bccc16f",
   "metadata": {},
   "source": [
    "#### TRANSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a61f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTransformer(nn.Module):\n",
    "    def __init__(self, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_proj=nn.Linear(2,hidden)\n",
    "        encoder_layer=nn.TransformerEncoderLayer(d_model=hidden,nhead=4,batch_first=True)\n",
    "        self.transformer= nn.TransformerEncoder(encoder_layer,num_layers=num_layers)\n",
    "        self.out_proj= nn.Linear(hidden,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x= self.input_proj(x)         \n",
    "        x= self.transformer(x)         \n",
    "        return self.out_proj(x[:, -1])   #last timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b410d66",
   "metadata": {},
   "source": [
    "# -------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c88d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model=OurTransformer(hidden,num_layers)\n",
    "optimizer=torch.optim.Adam(our_model.parameters(),lr=lr)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16e271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 1,rmse_x=8.185 μm,rmse_y=9.739 μm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m     l=loss(pred, target)\n\u001b[32m      9\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[43ml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     optimizer.step()\n\u001b[32m     13\u001b[39m our_model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/MI_research/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/MI_research/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/MI_research/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "rmse_list_x=[]\n",
    "rmse_list_y=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    our_model.train()\n",
    "    for inp,target in train_loader:\n",
    "        pred=our_model(inp)\n",
    "        l=loss(pred, target)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    our_model.eval()\n",
    "    errors=[]\n",
    "    with torch.no_grad():\n",
    "        for inp,target in test_loader:\n",
    "            pred= our_model(inp)\n",
    "            errors.append((pred-target).numpy())\n",
    "    errors=np.concatenate(errors)\n",
    "    rmse_x= np.sqrt((errors[:,0]**2).mean())\n",
    "    rmse_y= np.sqrt((errors[:,1]**2).mean())\n",
    "    rmse_list_x.append(rmse_x)\n",
    "    rmse_list_y.append(rmse_y)\n",
    "    print(f\"epoch number {epoch+1},rmse_x={rmse_x:.3f} μm,rmse_y={rmse_y:.3f} μm\")\n",
    "\n",
    "\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot()\n",
    "ax.plot(rmse_list_x,label=\"test x\")\n",
    "ax.plot(rmse_list_y,label=\"test y\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"training epoch\")\n",
    "ax.set_ylabel(\"test loss rmse\")\n",
    "ax.set_title(f\"test loss x & y \\n num_layers {num_layers} \\n hidden {hidden} \\n window {window} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
