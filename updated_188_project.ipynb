{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d622e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heres what the dataframe looks like btw:\n",
      "   Time (ms)  Centroid_X  Centroid_Y  Feature\n",
      "0          0  -18.112448   -7.618048       39\n",
      "1          1  -13.046840   -9.590575       39\n",
      "2          2   -8.764083  -12.284308       39\n",
      "3          3   -2.110503  -12.777150       39\n",
      "4          4    1.029483  -13.182605       39\n",
      "5          5    5.663209  -10.902855       39\n",
      "6          6    6.456484   -9.254714       39\n",
      "7          7    2.735900   -3.856508       39\n",
      "8          8   -0.484103   -0.866501       39\n",
      "9          9   -5.195110    3.019698       39\n",
      "[-18.112448 -13.04684   -8.764083 ... -21.287073 -22.94848  -22.354143]\n",
      "[ -7.618048   -9.590575  -12.284308  ...  -1.4127754  -2.2394783\n",
      "  -5.196636 ]\n",
      "[[-18.112448   -7.618048 ]\n",
      " [-13.04684    -9.590575 ]\n",
      " [ -8.764083  -12.284308 ]\n",
      " ...\n",
      " [-16.89624    14.82285  ]\n",
      " [-14.328587   12.191551 ]\n",
      " [-11.754089    3.9635835]]\n",
      "10000\n",
      "epoch number 1,test rmse_x=6.015 μm,test rmse_y=10.038 μm\n",
      "epoch number 11,test rmse_x=4.225 μm,test rmse_y=4.375 μm\n",
      "epoch number 21,test rmse_x=3.768 μm,test rmse_y=3.604 μm\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#data loading\n",
    "df= dataframe = pd.read_feather(\"combined_centroid_data.feather\")\n",
    "x=df[\"Centroid_X\"].values #.values is a pd attribute #gta take .values attribute to remove all the indexing, just returns array\n",
    "y=df[\"Centroid_Y\"].values\n",
    "our_data= np.stack([x,y], axis=1) #stack invents a new axis, we tryna couple the [x_i y_i]\n",
    "our_data = reduced_size= our_data[:10000] #the paper didnt use all the gazilliopn timesteps, theu used 10**5 i think. but it'll take way to long to run for our purposes, like several days.\n",
    "print(\"heres what the dataframe looks like btw:\")\n",
    "print(df[:10])\n",
    "print(x)\n",
    "print(y)\n",
    "print(our_data)\n",
    "print(len(our_data))\n",
    "\n",
    "#hyperparams to keep fixed\n",
    "epochs= 100 #if it doesnt converge, i gotta adjust\n",
    "batch_siz= 128 #just depending on speed mostly, big minib. faster\n",
    "lr=.002\n",
    "hidden=64 #REMEMBER THIS GOTTA B DIVIDISIBLE B nhead WHICH I PUT AS 4 FOR THRANDFORMER\n",
    "#lets create a class to return input and output data \n",
    "\n",
    "class OurJitterDataset():\n",
    "    def __init__(self,data,window,gap):\n",
    "        self.data=data\n",
    "        self.window=window\n",
    "        self.gap=gap\n",
    "\n",
    "    def __len__(self): #i use this s.t. i can run .random_split down below\n",
    "        return len(self.data)-self.window-self.gap #also this makes it so we dont go outside the data!\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        input= torch.tensor(self.data[i:i+self.window], dtype=torch.float32) #thisll b 600,2\n",
    "        output=torch.tensor( self.data[i+self.window+self.gap], dtype=torch.float32) #thisll b 2,. the output prediction\n",
    "        return input,output\n",
    "    \n",
    "# MODELS ########################\n",
    "#### LSTM\n",
    "#lets roll out the model\n",
    "class OurLSTM(nn.Module):\n",
    "    def __init__(self, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm=nn.LSTM(2,hidden_size=hidden, num_layers=num_layers,batch_first=True)\n",
    "        self.out_proj=nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output_of_the_lstm,(final_hidden_state,more_irrelevant_stuff)= self.lstm(x)  #\n",
    "        return self.out_proj(final_hidden_state[-1]) #last layer hidden state, equiv to [0] for us cuz we just have 1 layer\n",
    "#### LSTM+CNN\n",
    "class OurCNNLSTM(nn.Module):\n",
    "    def __init__(self, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.conv= nn.Conv1d(2,32,kernel_size=5,padding=2)  \n",
    "        self.lstm= nn.LSTM(32,hidden_size=hidden,num_layers=num_layers,batch_first=True)\n",
    "        self.out_proj= nn.Linear(hidden, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x= x.permute(0,2,1)        \n",
    "        x= torch.relu(self.conv(x))  \n",
    "        x= x.permute(0,2,1)      \n",
    "        output_of_the_lstm,(final_hidden_state,more_irrelevant_stuff)= self.lstm(x)\n",
    "        return self.out_proj(final_hidden_state[-1])\n",
    "#### TRANSF\n",
    "class OurTransformer(nn.Module):\n",
    "    def __init__(self, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_proj=nn.Linear(2,hidden)\n",
    "        encoder_layer=nn.TransformerEncoderLayer(d_model=hidden,nhead=4,batch_first=True)\n",
    "        self.transformer= nn.TransformerEncoder(encoder_layer,num_layers=num_layers)\n",
    "        self.out_proj= nn.Linear(hidden,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x= self.input_proj(x)         \n",
    "        x= self.transformer(x)         \n",
    "        return self.out_proj(x[:, -1])   #last timestep\n",
    "                    \n",
    "\n",
    "\n",
    "#the 3 hyperparams to vary\n",
    "for gap in [5,10,20]:\n",
    "    for window in [150,300,600]:\n",
    "\n",
    "        #load up the data. we can put it here bc it only depends on gap and window, not num layers in the model\n",
    "        our_dataset = OurJitterDataset(our_data,window,gap)\n",
    "        train_size=int(.8*len(our_dataset))\n",
    "            \n",
    "        train_data,test_data= torch.utils.data.random_split(our_dataset,[train_size,len(our_dataset)-train_size])\n",
    "        train_loader= DataLoader(train_data,batch_size=batch_siz,shuffle=True)\n",
    "        test_loader= DataLoader(test_data,batch_size=batch_siz,shuffle=True)\n",
    "\n",
    "        for num_layers in [1,2,3]:\n",
    "            for model_class in [OurLSTM, OurCNNLSTM , OurTransformer]:\n",
    "\n",
    "                our_model = model_class(hidden,num_layers) #instantiating and initializing the model based on the model class which we loop over\n",
    "                # -------------------------------------------\n",
    "                ###########################################here!!!!!!!!!!\n",
    "                optimizer=torch.optim.Adam(our_model.parameters(),lr=lr)\n",
    "                loss_fn = nn.MSELoss()\n",
    "\n",
    "                train_rmse_list_x=[]\n",
    "                train_rmse_list_y=[]\n",
    "                test_rmse_list_x=[]\n",
    "                test_rmse_list_y=[]\n",
    "                train_loss_list=[]\n",
    "                test_loss_list=[]\n",
    "\n",
    "                #NOTICE! a thing to be mindful of here is that the model mse is taken per batch whereas rmse x list is taken per epoch... i will keep it like this because we dont need the model mse anyway, its just extra information, and rmse is more interpretable for our task anyway!\n",
    "\n",
    "                for epoch in range(epochs):\n",
    "                    our_model.train()\n",
    "                    for inp,target in train_loader:\n",
    "                        pred=our_model(inp)\n",
    "                        l=loss_fn(pred, target)\n",
    "                        train_loss_list.append(l.item()) #.item() gets the value... bc remember, l will have gradient attched to it\n",
    "                        optimizer.zero_grad()\n",
    "                        l.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "\n",
    "                    our_model.eval() #so we disable dropout\n",
    "                    \n",
    "                    train_errors=[] #this is rmse\n",
    "                    test_errors=[] #this is rmse\n",
    "\n",
    "                    #lets look at the rmse\n",
    "                    with torch.no_grad(): #just makes the runs a little faster by disabling the whole computational graph stuff. it matters alot actually when u run big models.\n",
    "                        for inp,target in train_loader:\n",
    "                            pred= our_model(inp)\n",
    "                            train_errors.append((pred-target).numpy())\n",
    "                    train_errors=np.concatenate(train_errors)\n",
    "                    rmse_x= np.sqrt((train_errors[:,0]**2).mean())\n",
    "                    rmse_y= np.sqrt((train_errors[:,1]**2).mean())\n",
    "                    train_rmse_list_x.append(rmse_x)\n",
    "                    train_rmse_list_y.append(rmse_y)\n",
    "\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for inp,target in test_loader:\n",
    "                            pred= our_model(inp)\n",
    "                            test_errors.append((pred-target).numpy())\n",
    "                            test_loss_list.append(loss_fn(pred,target).item())\n",
    "                    test_errors=np.concatenate(test_errors)\n",
    "                    rmse_x= np.sqrt((test_errors[:,0]**2).mean())\n",
    "                    rmse_y= np.sqrt((test_errors[:,1]**2).mean())\n",
    "                    test_rmse_list_x.append(rmse_x)\n",
    "                    test_rmse_list_y.append(rmse_y)\n",
    "                    if epoch%10==0:\n",
    "                        print(f\"epoch number {epoch+1},test rmse_x={rmse_x:.3f} μm,test rmse_y={rmse_y:.3f} μm\")\n",
    "\n",
    "                fig=plt.figure()\n",
    "                ax=fig.add_subplot()\n",
    "                ax.plot(train_rmse_list_x,label=\"train x\")\n",
    "                ax.plot(train_rmse_list_y,label=\"train y\")\n",
    "                ax.plot(test_rmse_list_x,label=\"test x\")\n",
    "                ax.plot(test_rmse_list_y,label=\"test y\")\n",
    "                ax.legend()\n",
    "                ax.set_xlabel(\"training epoch\")\n",
    "                ax.set_ylabel(\"test loss rmse\")\n",
    "                ax.set_title(f\"rmse of testing data for x and y \\n model: {model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\")\n",
    "                \n",
    "                #all the plots!!!! all plots are gonna get saved into the directory, so the folder we store this file in \n",
    "                plt.savefig(f\"plot_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\")                        #turns out u gotta save before showing lest u get an empty plot. also use the name dunders to avoid those classic ugly brackets u get othewise\n",
    "                #lets save the trained model. lemme just give it unmistakeable names:\n",
    "                torch.save(our_model.state_dict(), f\"params_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\")\n",
    "\n",
    "\n",
    "                np.save(f\"train_rmse_x_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\", train_rmse_list_x)\n",
    "                np.save(f\"train_rmse_y_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\", train_rmse_list_y)\n",
    "                np.save(f\"test_rmse_x_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\", test_rmse_list_x)\n",
    "                np.save(f\"test_rmse_y_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\", test_rmse_list_y)\n",
    "                np.save(f\"train_loss_list_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\", train_loss_list) #we can also plot this later shud we find it interesting. but rn lets not overpower ourselves, rmse is what ewe care about anyway. also these guys are gonna be per batchw whereas the rmse guys are per epoch\n",
    "                np.save(f\"test_loss_list_{model_class.__name__}_gap{gap}_window{window}_num_layers{num_layers}\", test_loss_list)\n",
    "\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f06911",
   "metadata": {},
   "source": [
    "# below is the same code, except in the way we originally wrote it, which is ipynb format. code above however runs grid search loop, hence all in one cell (the in loop above also we have added a few things things like torch.save(), plt.save() etc.) also just load w np.load etc the saved stuff from above so we can put it in our paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d8d52",
   "metadata": {},
   "source": [
    "### experimenting with various architectures beyond MLP :-)\n",
    "##############################################################\n",
    "#### 1. LSTM\n",
    "#### 2. CNN + LSTM\n",
    "#### 3. Transformer\n",
    "##############################################################\n",
    "### for each of these, i'll vary: \n",
    "#### 1. num_layers\n",
    "#### 2. hidden size (aka dimensionality)\n",
    "#### 3. Input window length (150,300,600)... it could be that too many time-samples cause overfitting\n",
    "##############################################################\n",
    "#####   technically, we could do a grid search and try like 3 combos of each hyperparam, i.e. 3x3x3=27 runs for each architecture type\n",
    "#####   however, i'll just run \"fix and vary\". it's unlikely that there are interactions here between the hyperparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a04de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#data loading\n",
    "df= dataframe = pd.read_feather(\"combined_centroid_data.feather\")\n",
    "x=df[\"Centroid_X\"].values #.values is a pd attribute #gta take .values attribute to remove all the indexing, just returns array\n",
    "y=df[\"Centroid_Y\"].values\n",
    "our_data= np.stack([x,y], axis=1) #stack invents a new axis, we tryna couple the [x_i y_i]\n",
    "our_data = reduced_size= our_data[:10000] #the paper didnt use all the gazilliopn timesteps, theu used 10**5 i think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaeedb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heres what the dataframe looks like btw:\n",
      "   Time (ms)  Centroid_X  Centroid_Y  Feature\n",
      "0          0  -18.112448   -7.618048       39\n",
      "1          1  -13.046840   -9.590575       39\n",
      "2          2   -8.764083  -12.284308       39\n",
      "3          3   -2.110503  -12.777150       39\n",
      "4          4    1.029483  -13.182605       39\n",
      "5          5    5.663209  -10.902855       39\n",
      "6          6    6.456484   -9.254714       39\n",
      "7          7    2.735900   -3.856508       39\n",
      "8          8   -0.484103   -0.866501       39\n",
      "9          9   -5.195110    3.019698       39\n",
      "[-18.112448 -13.04684   -8.764083 ... -21.287073 -22.94848  -22.354143]\n",
      "[ -7.618048   -9.590575  -12.284308  ...  -1.4127754  -2.2394783\n",
      "  -5.196636 ]\n",
      "[[-18.112448    -7.618048  ]\n",
      " [-13.04684     -9.590575  ]\n",
      " [ -8.764083   -12.284308  ]\n",
      " ...\n",
      " [ -0.63192445   1.02285   ]\n",
      " [  3.8037598    1.02285   ]\n",
      " [  8.485918     0.28353426]]\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(\"heres what the dataframe looks like btw:\")\n",
    "print(df[:10])\n",
    "print(x)\n",
    "print(y)\n",
    "print(our_data)\n",
    "print(len(our_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams to keep fixed\n",
    "epochs= 100 #if it doesnt converge, i gotta adjust\n",
    "batch_siz= 128 #just depending on speed mostly, big minib. faster\n",
    "gap=20\n",
    "lr=.002\n",
    "\n",
    "#the 3 hyperparams to vary\n",
    "num_layers=3\n",
    "hidden=64 #REMEMBER THIS GOTTA B DIVIDISIBLE B nhead WHICH I PUT AS 4 FOR THRANDFORMER\n",
    "window=600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15289c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create a class to return input and output data \n",
    "\n",
    "class OurJitterDataset():\n",
    "    def __init__(self,data,window,gap):\n",
    "        self.data=data\n",
    "        self.window=window\n",
    "        self.gap=gap\n",
    "\n",
    "    def __len__(self): #i use this s.t. i can run .random_split down below\n",
    "        return len(self.data)-self.window-self.gap #also this makes it so we dont go outside the data!\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        input= torch.tensor(self.data[i:i+self.window], dtype=torch.float32) #thisll b 600,2\n",
    "        output=torch.tensor( self.data[i+self.window+self.gap], dtype=torch.float32) #thisll b 2,. the output prediction\n",
    "        return input,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33816122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load up the data\n",
    "our_dataset = OurJitterDataset(our_data,window,gap)\n",
    "train_size=int(.8*len(our_dataset))\n",
    "    \n",
    "train_data,test_data= torch.utils.data.random_split(our_dataset,[train_size,len(our_dataset)-train_size])\n",
    "train_loader= DataLoader(train_data,batch_size=batch_siz,shuffle=True)\n",
    "test_loader= DataLoader(test_data,batch_size=batch_siz,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc3b2f8",
   "metadata": {},
   "source": [
    "# MODELS ########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e44b7",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa6ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets roll out the model\n",
    "class OurLSTM(nn.Module):\n",
    "    def __init__(self, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm=nn.LSTM(2,hidden_size=hidden, num_layers=num_layers,batch_first=True)\n",
    "        self.out_proj=nn.Linear(hidden,2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output_of_the_lstm,(final_hidden_state,more_irrelevant_stuff)= self.lstm(x)  #\n",
    "        return self.out_proj(final_hidden_state[-1]) #last layer hidden state, equiv to [0] for us cuz we just have 1 layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35bd576",
   "metadata": {},
   "source": [
    "#### LSTM+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe13b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurCNNLSTM(nn.Module):\n",
    "    def __init__(self, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.conv= nn.Conv1d(2,32,kernel_size=5,padding=2)  \n",
    "        self.lstm= nn.LSTM(32,hidden_size=hidden,num_layers=num_layers,batch_first=True)\n",
    "        self.out_proj= nn.Linear(hidden, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x= x.permute(0,2,1)        \n",
    "        x= torch.relu(self.conv(x))  \n",
    "        x= x.permute(0,2,1)      \n",
    "        output_of_the_lstm,(final_hidden_state,more_irrelevant_stuff)= self.lstm(x)\n",
    "        return self.out_proj(final_hidden_state[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bccc16f",
   "metadata": {},
   "source": [
    "#### TRANSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a61f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTransformer(nn.Module):\n",
    "    def __init__(self, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_proj=nn.Linear(2,hidden)\n",
    "        encoder_layer=nn.TransformerEncoderLayer(d_model=hidden,nhead=4,batch_first=True)\n",
    "        self.transformer= nn.TransformerEncoder(encoder_layer,num_layers=num_layers)\n",
    "        self.out_proj= nn.Linear(hidden,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x= self.input_proj(x)         \n",
    "        x= self.transformer(x)         \n",
    "        return self.out_proj(x[:, -1])   #last timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b410d66",
   "metadata": {},
   "source": [
    "# -------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c88d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model=OurTransformer(hidden,num_layers)\n",
    "optimizer=torch.optim.Adam(our_model.parameters(),lr=lr)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16e271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 1,rmse_x=8.185 μm,rmse_y=9.739 μm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m     l=loss(pred, target)\n\u001b[32m      9\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[43ml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     optimizer.step()\n\u001b[32m     13\u001b[39m our_model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/MI_research/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/MI_research/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/MI_research/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "rmse_list_x=[]\n",
    "rmse_list_y=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    our_model.train()\n",
    "    for inp,target in train_loader:\n",
    "        pred=our_model(inp)\n",
    "        l=loss(pred, target)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    our_model.eval()\n",
    "    errors=[]\n",
    "    with torch.no_grad():\n",
    "        for inp,target in test_loader:\n",
    "            pred= our_model(inp)\n",
    "            errors.append((pred-target).numpy())\n",
    "    errors=np.concatenate(errors)\n",
    "    rmse_x= np.sqrt((errors[:,0]**2).mean())\n",
    "    rmse_y= np.sqrt((errors[:,1]**2).mean())\n",
    "    rmse_list_x.append(rmse_x)\n",
    "    rmse_list_y.append(rmse_y)\n",
    "    print(f\"epoch number {epoch+1},rmse_x={rmse_x:.3f} μm,rmse_y={rmse_y:.3f} μm\")\n",
    "\n",
    "\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot()\n",
    "ax.plot(rmse_list_x,label=\"test x\")\n",
    "ax.plot(rmse_list_y,label=\"test y\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"training epoch\")\n",
    "ax.set_ylabel(\"test loss rmse\")\n",
    "ax.set_title(f\"test loss x & y \\n num_layers {num_layers} \\n hidden {hidden} \\n window {window} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MI_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
