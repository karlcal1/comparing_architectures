{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75d8d52",
   "metadata": {},
   "source": [
    "### experimenting with various architectures beyond MLP :-)\n",
    "##############################################################\n",
    "#### 1. LSTM\n",
    "#### 2. CNN + LSTM\n",
    "#### 3. Transformer\n",
    "##############################################################\n",
    "### for each of these, i'll vary: \n",
    "#### 1. num_layers\n",
    "#### 2. hidden size (aka dimensionality)\n",
    "#### 3. Input window length (150,300,600)... it could be that too many time-samples cause overfitting\n",
    "##############################################################\n",
    "#####   technically, we could do a grid search and try like 3 combos of each hyperparam, i.e. 3x3x3=27 runs for each architecture type\n",
    "#####   however, i'll just run \"fix and vary\". it's unlikely that there are interactions here between the hyperparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4a04de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#data loading\n",
    "df= dataframe = pd.read_feather(\"combined_centroid_data.feather\")\n",
    "x=df[\"Centroid_X\"].values #.values is a pd attribute #gta take .values attribute to remove all the indexing, just returns array\n",
    "y=df[\"Centroid_Y\"].values\n",
    "our_data= np.stack([x,y], axis=1) #stack invents a new axis, we tryna couple the [x_i y_i]\n",
    "our_data = reduced_size= our_data[:2000] #the paper didnt use all the gazilliopn timesteps, theu used 10**5 i think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaaeedb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heres what the dataframe looks like btw:\n",
      "   Time (ms)  Centroid_X  Centroid_Y  Feature\n",
      "0          0  -18.112448   -7.618048       39\n",
      "1          1  -13.046840   -9.590575       39\n",
      "2          2   -8.764083  -12.284308       39\n",
      "3          3   -2.110503  -12.777150       39\n",
      "4          4    1.029483  -13.182605       39\n",
      "5          5    5.663209  -10.902855       39\n",
      "6          6    6.456484   -9.254714       39\n",
      "7          7    2.735900   -3.856508       39\n",
      "8          8   -0.484103   -0.866501       39\n",
      "9          9   -5.195110    3.019698       39\n",
      "[-18.112448 -13.04684   -8.764083 ... -21.287073 -22.94848  -22.354143]\n",
      "[ -7.618048   -9.590575  -12.284308  ...  -1.4127754  -2.2394783\n",
      "  -5.196636 ]\n",
      "[[-18.112448    -7.618048  ]\n",
      " [-13.04684     -9.590575  ]\n",
      " [ -8.764083   -12.284308  ]\n",
      " ...\n",
      " [ 12.165915    -2.2464797 ]\n",
      " [  9.042353    -0.05032654]\n",
      " [  9.042353     0.61739504]]\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(\"heres what the dataframe looks like btw:\")\n",
    "print(df[:10])\n",
    "print(x)\n",
    "print(y)\n",
    "print(our_data)\n",
    "print(len(our_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be18c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams to keep fixed\n",
    "EPOCHS= 100 #if it doesnt converge, i gotta adjust\n",
    "BATCH_SIZE= 128 #just depending on speed mostly, big minib. faster\n",
    "GAP=20\n",
    "\n",
    "#the 3 hyperparams to vary\n",
    "NUM_LAYERS=1\n",
    "HIDDEN=64 #REMEMBER THIS GOTTA B DIVIDISIBLE B nhead WHICH I PUT AS 4 FOR THRANDFORMER\n",
    "WINDOW=600 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15289c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create a class to return input and output data \n",
    "\n",
    "class OurJitterDataset(Dataset):\n",
    "    def __init__(self, data, window, gap):\n",
    "        self.data=data\n",
    "        self.window=window\n",
    "        self.gap=gap\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-self.window-self.gap\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        inputt= self.data[i:i+self.window] #thisll b 600,2\n",
    "        outputt=self.data[i+self.window+self.gap] #thisll b 2,\n",
    "        return torch.tensor(inputt,dtype=torch.float32),torch.tensor(outputt,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33816122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load up the data\n",
    "our_dataset = OurJitterDataset(our_data,WINDOW,GAP)\n",
    "train_size=int(.8*len(our_dataset))\n",
    "    #randomized_data = np.random.permutation(our_dataset) #theres a torch fn 4 this but i forgot it\n",
    "    #train_data,test_data = randomized_data[:train_size], randomized_data[train_size:]\n",
    "train_data, test_data = torch.utils.data.random_split(our_dataset, [train_size, len(our_dataset) - train_size])\n",
    "train_loader= DataLoader(train_data,batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader= DataLoader(test_data,batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc3b2f8",
   "metadata": {},
   "source": [
    "# MODELS ########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e44b7",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49fa6ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets roll out the model\n",
    "\n",
    "class OurLSTM(nn.Module):\n",
    "    def __init__(self, HIDDEN, NUM_LAYERS):\n",
    "        super().__init__()\n",
    "        self.lstm=nn.LSTM(2,hidden_size=HIDDEN, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.out_proj=nn.Linear(HIDDEN,2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        _,(h,_) = self.lstm(x)  #\n",
    "        return self.out_proj(h[-1]) #last layer hidden state!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35bd576",
   "metadata": {},
   "source": [
    "#### LSTM+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe13b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurCNNLSTM(nn.Module):\n",
    "    def __init__(self, HIDDEN, NUM_LAYERS):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(2, 32, kernel_size=5, padding=2)  # (batch, 2, 600) -> (batch, 32, 600)\n",
    "        self.lstm = nn.LSTM(32, hidden_size=HIDDEN, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.out_proj = nn.Linear(HIDDEN, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)        # (batch, 600, 2) -> (batch, 2, 600) for conv1d\n",
    "        x = torch.relu(self.conv(x))  # (batch, 32, 600)\n",
    "        x = x.permute(0, 2, 1)        # (batch, 600, 32) for lstm\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        return self.out_proj(h[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bccc16f",
   "metadata": {},
   "source": [
    "#### TRANSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db1a61f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTransformer(nn.Module):\n",
    "    def __init__(self, HIDDEN, NUM_LAYERS):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(2, HIDDEN)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=HIDDEN, nhead=4, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=NUM_LAYERS)\n",
    "        self.out_proj = nn.Linear(HIDDEN, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)          # (batch, 600, 2) -> (batch, 600, HIDDEN)\n",
    "        x = self.transformer(x)          # (batch, 600, HIDDEN)\n",
    "        return self.out_proj(x[:, -1])   # take last timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b410d66",
   "metadata": {},
   "source": [
    "# -------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76c88d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model=OurLSTM(HIDDEN,NUM_LAYERS)\n",
    "optimizer=torch.optim.Adam(our_model.parameters(),lr=.02)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_list_x=[]\n",
    "rmse_list_y=[]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    our_model.train()\n",
    "    for inp, target in train_loader:\n",
    "        pred = our_model(inp)\n",
    "        l = loss(pred, target)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    our_model.eval()\n",
    "    errors = []\n",
    "    with torch.no_grad():\n",
    "        for inp, target in test_loader:\n",
    "            pred = our_model(inp)\n",
    "            errors.append((pred - target).numpy())\n",
    "    errors = np.concatenate(errors)\n",
    "    rmse_x = np.sqrt((errors[:, 0] ** 2).mean())\n",
    "    rmse_y = np.sqrt((errors[:, 1] ** 2).mean())\n",
    "    rmse_list_x.append(rmse_x)\n",
    "    rmse_list_y.append(rmse_y)\n",
    "    print(f\"Epoch {epoch+1}: RMSE_x={rmse_x:.3f} μm, RMSE_y={rmse_y:.3f} μm\")\n",
    "\n",
    "\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot()\n",
    "ax.plot(rmse_list_x,label=\"test x\")\n",
    "ax.plot(rmse_list_y,label=\"test y\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"training epoch\")\n",
    "ax.set_ylabel(\"test loss rmse\")\n",
    "ax.set_title(f\"test loss x & y \\n NUM_LAYERS {NUM_LAYERS} \\n HIDDEN {HIDDEN} \\n WINDOW {WINDOW} \")\n",
    "\n",
    "\n",
    "NUM_LAYERS=1\n",
    "HIDDEN=64 #REMEMBER THIS GOTTA B DIVIDISIBLE B nhead WHICH I PUT AS 4 FOR THRANDFORMER\n",
    "WINDOW=600 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6123f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ca2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf98ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MI_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
